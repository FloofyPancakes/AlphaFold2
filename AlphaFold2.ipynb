{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedec855-48c9-47df-9a96-c99b6e62656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sidechainnet as scn\n",
    "import random\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80adece-2499-45bb-9275-0632cbdaf960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SidechainNet was loaded from ./sidechainnet_data/sidechainnet_casp7_30.pkl.\n"
     ]
    }
   ],
   "source": [
    "data = scn.load(casp_version=7, with_pytorch=\"dataloaders\", \n",
    "                seq_as_onehot=True, aggregate_model_input=False,\n",
    "               batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94d25bc-a054-46b4-bbb3-c7a5945394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_features(batch):\n",
    "    '''\n",
    "    Take a batch of sequence info and return the sequence (one-hot),\n",
    "    evolutionary info and (phi, psi, omega) angles per position, \n",
    "    as well as position mask.\n",
    "    Also return the distance matrix, and distance mask.\n",
    "    '''\n",
    "    str_seqs = batch.str_seqs # seq in str format\n",
    "    seqs = batch.seqs # seq in one-hot format\n",
    "    int_seqs = batch.int_seqs # seq in int format\n",
    "    masks = batch.msks # which positions are valid\n",
    "    lengths = batch.lengths # seq length\n",
    "    evos = batch.evos # PSSM / evolutionary info\n",
    "    angs = batch.angs[:,:,0:2] # torsion angles: phi, psi\n",
    "    \n",
    "    # use coords to create distance matrix from c-beta\n",
    "    # except use c-alpha for G\n",
    "    # coords[:, 4, :] is c-beta, and coords[:, 1, :] is c-alpha\n",
    "    coords = batch.crds # seq coord info (all-atom)\n",
    "    batch_xyz = []\n",
    "    for i in range(coords.shape[0]):\n",
    "        xyz = []\n",
    "        xyz = [coords[i][cpos+4,:] \n",
    "                if masks[i][cpos//14] and str_seqs[i][cpos//14] != 'G'\n",
    "                else coords[i][cpos+1,:]\n",
    "                for cpos in range(0, coords[i].shape[0]-1, 14)]\n",
    "        batch_xyz.append(torch.stack(xyz))\n",
    "    batch_xyz = torch.stack(batch_xyz)\n",
    "    # now create pairwise distance matrix\n",
    "    dmats = torch.cdist(batch_xyz, batch_xyz)\n",
    "    # create matrix mask (0 means i,j invalid)\n",
    "    dmat_masks = torch.einsum('bi,bj->bij', masks, masks)\n",
    "    \n",
    "    return seqs, evos, angs, masks, dmats, dmat_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6969eac3-0163-479e-bc59-ae546aa2d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads, c, c_z, c_m):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.c = c\n",
    "        self.c_z = c_z\n",
    "        \n",
    "        #query key value\n",
    "        self.q = nn.Linear(c_m, self.c, bias = False)\n",
    "        self.k = nn.Linear(c_m, self.c, bias = False)\n",
    "        self.v = nn.Linear(c_m, self.c, bias = False)\n",
    "        \n",
    "        #bias projects z from 128 to 1\n",
    "        self.bias = nn.Linear(self.c_z, 1, bias = False)\n",
    "        \n",
    "    def forward(self, msa_rep, pair_rep, row):\n",
    "        #get query key value\n",
    "        query = self.q(msa_rep)\n",
    "        key = self.k(msa_rep)\n",
    "        value = self.v(msa_rep)\n",
    "        \n",
    "        out = torch.matmul(query,torch.transpose(key,1,2))/np.sqrt(self.c)\n",
    "        if row: \n",
    "            b = self.bias(pair_rep).squeeze()\n",
    "            out += b\n",
    "        \n",
    "        #softmax with respect to rows\n",
    "        out = F.softmax(out, dim = 1)\n",
    "        out = torch.matmul(out, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd7ddac-b3c4-4539-b0ce-8a46314de992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rowColAtt(nn.Module):\n",
    "    def __init__(self, c, c_m, c_z, num_heads, row, device):\n",
    "        super(rowColAtt, self).__init__()\n",
    "        \n",
    "        self.row = row\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        \n",
    "        #attention heads\n",
    "        self.mhsa = nn.ModuleList([AttentionHead(num_heads, c, c_z, c_m) for i in range(self.num_heads)])\n",
    "        \n",
    "        #project to c_msa\n",
    "        self.fc1 = nn.Linear(c_z, c_m)\n",
    "        \n",
    "        self.gate = nn.ModuleList([nn.Sequential(nn.Linear(c_m, 1), nn.Sigmoid()) for i in range(self.num_heads)])\n",
    "        \n",
    "    def forward(self, msa_rep, pair_rep):\n",
    "        new_msa_rep = torch.empty(msa_rep.shape).to(self.device)\n",
    "        for s in range(msa_rep.shape[1]):\n",
    "            s_o = []\n",
    "            for i, head in enumerate(self.mhsa):\n",
    "                out = head(msa_rep[:,s,:,:], pair_rep, self.row)\n",
    "                gate = self.gate[i](msa_rep).squeeze(dim=-1)\n",
    "                final = torch.transpose(gate, 1, 2) * out       #gate is b x n_clust x n_res, out is b x n_res x c, cannot be dotted, might skew results\\n\",\n",
    "                s_o.append(final)\n",
    "                \n",
    "            new_slice = torch.concat(s_o, dim = 2)\n",
    "            new_slice = self.fc1(new_slice)\n",
    "            new_msa_rep[:,s,:,:] = new_slice\n",
    "            \n",
    "        return new_msa_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e4847d-665b-48b7-b762-30e7a379d46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Outer_Prod_Mean(nn.Module):\n",
    "    '''\n",
    "    Finds the outer product mean between the pair-wise representation\n",
    "    and the msa representation.\n",
    "    The output is a n_res x n_res x 128 pair_rep\n",
    "    '''\n",
    "    def __init__(self, c, c_m, c_z, device):\n",
    "        super(Outer_Prod_Mean, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        #linear layer to project i[s] and j[s] to c dim\n",
    "        self.fc1 = nn.Linear(c_m, c)\n",
    "        self.fc2 = nn.Linear(c_m, c)\n",
    "        \n",
    "        #flatten the mean outer product to C*C\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #linear layer to project the outer product mean to 128 dim\n",
    "        self.fc3 = nn.Linear(c_m, c_z)\n",
    "        \n",
    "    def forward(self, msa_rep, pair_rep):\n",
    "        #project m to A and B\n",
    "        new_pair_rep = torch.empty(pair_rep.shape).to(self.device)\n",
    "        \n",
    "        for i in range(msa_rep.shape[1]):\n",
    "            for j in range(msa_rep.shape[1]):\n",
    "                a = self.fc1(msa_rep[:,i,:,:])\n",
    "                b = self.fc2(msa_rep[:,j,:,:])\n",
    "                outer =torch.einsum('bij,bik->bijk',a,b)\n",
    "                out_mean = torch.mean(outer, dim = 1)\n",
    "                out = self.flatten(out_mean)\n",
    "                out = self.fc3(out)\n",
    "                new_pair_rep[:,i,j,:] = out\n",
    "\n",
    "        return new_pair_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c25d7-afaa-4f00-a41f-e22aa3479522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mult_Attention(nn.Module):\n",
    "    def __init__(self, c_z, c_m, out, device):\n",
    "        '''\n",
    "        Does incoming(default) multiplicative attention on a given pair_rep.\n",
    "        out: set to False to do incoming attention\n",
    "        '''\n",
    "        super(Mult_Attention, self).__init__()\n",
    "        self.device = device\n",
    "        self.out = out\n",
    "        self.ln = nn.LayerNorm(c_z)\n",
    "        self.fc1 = nn.Linear(c_z, c_z)\n",
    "        self.fc2 = nn.Linear(c_z, c_z)\n",
    "        self.fc3 = nn.Linear(c_m, c_z)\n",
    "\n",
    "        self.gate1 = nn.Sequential(nn.Linear(c_z, c_z), nn.Sigmoid())\n",
    "        self.gate2 = nn.Sequential(nn.Linear(c_z, c_z), nn.Sigmoid())\n",
    "        self.gate3 = nn.Sequential(nn.Linear(c_z, c_m), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, pair_rep):\n",
    "        # print(pair_rep.shape)\n",
    "        #Do a layer norm on pair_rep\n",
    "        pair_rep = self.ln(pair_rep)\n",
    "        Z = torch.zeros((pair_rep.shape[0], pair_rep.shape[1], pair_rep.shape[2], c_m)).to(self.device)\n",
    "        #make A and B\n",
    "        A = self.fc1(pair_rep)\n",
    "        B = self.fc2(pair_rep)\n",
    "\n",
    "        #Make gates for A and B\n",
    "        gate_A = self.gate1(pair_rep)\n",
    "        gate_B = self.gate2(pair_rep)\n",
    "        gate_Z = self.gate3(pair_rep)\n",
    "\n",
    "        #take dot product of A, B and their gates\n",
    "        new_A = A * gate_A\n",
    "        new_B = B * gate_B\n",
    "\n",
    "        #transpose a and b if we are doing incoming attention\n",
    "        if not self.out:\n",
    "            new_A = torch.transpose(new_A, 1, 2)\n",
    "            new_B = torch.transpose(new_B, 1, 2)\n",
    "\n",
    "        for i in range(new_A.shape[1]):\n",
    "            for j in range(new_B.shape[2]):\n",
    "                Z[:,i,j] = gate_Z[:,i,j] * (torch.sum((new_A[:,i,:] * new_B[:,j,:]), dim = -1))\n",
    "\n",
    "        return self.fc3(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd194cd3-79af-4562-b175-a5231d924968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tri_Attention(nn.Module):\n",
    "    '''\n",
    "    Does starting triangular attention by default.\n",
    "    ending: set to true to do ending triangular attention\n",
    "    '''\n",
    "    def __init__(self, c, c_z, ending = False, num_heads = 4):\n",
    "        super(Tri_Attention, self).__init__()\n",
    "        self.ending = ending\n",
    "        self.num_heads = num_heads\n",
    "        self.c = c\n",
    "        \n",
    "        self.q = nn.ModuleList([nn.Linear(c_z, c) for i in range(num_heads)])\n",
    "        self.k = nn.ModuleList([nn.Linear(c_z, c) for i in range(num_heads)])\n",
    "        self.v = nn.ModuleList([nn.Linear(c_z, c) for i in range(num_heads)])\n",
    "        self.b = nn.ModuleList([nn.Linear(c_z, 1) for i in range(num_heads)])\n",
    "        self.g = nn.ModuleList([nn.Sequential(nn.Linear(c_z,c), nn.Sigmoid()) for i in range(num_heads)])\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, c_z)\n",
    "        \n",
    "    def forward(self, pair_rep):\n",
    "        output = []\n",
    "        for h in range(self.num_heads):\n",
    "            query = self.q[h](pair_rep)\n",
    "            key = self.k[h](pair_rep)\n",
    "            value = self.v[h](pair_rep)\n",
    "            bias = self.b[h](pair_rep)\n",
    "            gate = self.g[h](pair_rep)\n",
    "            \n",
    "            #find attention\n",
    "            a = (query * key)/np.sqrt(self.c) + bias\n",
    "            a = F.softmax(a, dim = -1)\n",
    "            a = a * value\n",
    "            out = a * gate\n",
    "            output.append(out)\n",
    "        \n",
    "        #concat all outputs\n",
    "        output = torch.concat(output, -1)\n",
    "        output = self.fc1(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df0f741-c8fc-4e5e-858b-6a635e630516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evoformer(nn.Module):\n",
    "    def __init__(self, c, c_m, c_z, n_clust, num_heads, device):\n",
    "        '''\n",
    "        Creates the MSA_representation and the Z(pairwise) matrix given a PSSM and a sequence.\n",
    "        n_clust: number of PSSMs.\n",
    "        num_heads: number of attention heads(8 by default)\n",
    "        '''\n",
    "        super(Evoformer, self).__init__()\n",
    "        \n",
    "        self.n_clust = n_clust\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        \n",
    "        #linear layers to project evos into n_clust x n_res x 256\n",
    "        self.fc0 = nn.ModuleList([nn.Linear(21, c_m) for i in range(n_clust)])\n",
    "        #linear layer to project seqs to n_res x 256\n",
    "        self.fc1 = nn.Linear(20, c_m)\n",
    "        #linear layer to project seqs to n_res x 128\n",
    "        self.fc2 = nn.Linear(20, c_z)\n",
    "        self.fc3 = nn.Linear(20, c_z)\n",
    "        #Linear layer to project distances into 128 space\n",
    "        self.fc4 = nn.Linear(64, c_z)\n",
    "        #linear layer to project pair_rep to bias\n",
    "        self.fc5 = nn.Linear(c_z, 1)\n",
    "        #linear layer to project the single representation to 256 dim\n",
    "        self.fc6 = nn.Linear(c_m, c_m)\n",
    "        #linear layer to project the single representation to 384 dim\n",
    "        self.fc7 = nn.Linear(c_m, 384)\n",
    "        \n",
    "        #define the transitional layers to pass the new msa_rep through\n",
    "        self.transition1 = nn.Sequential(nn.Linear(c_m, 4*c_m), nn.ReLU(), nn.Linear(4*c_m, c_m))\n",
    "        self.transition2 = nn.Sequential(nn.Linear(c_z, 4*c_z), nn.ReLU(), nn.Linear(4*c_z, c_z))\n",
    "        \n",
    "        #define all attentions\n",
    "        self.row_att = rowColAtt(c, c_m, c_z, self.num_heads, True, self.device)\n",
    "        self.col_att = rowColAtt(c, c_m, c_z, self.num_heads, False, self.device)\n",
    "        self.mul_att_in = Mult_Attention(c_z, c_m, False, device)\n",
    "        self.mul_att_out = Mult_Attention(c_z, c_m, True, device)\n",
    "        self.tri_att_start = Tri_Attention(c, c_z, ending = False)\n",
    "        self.tri_att_end = Tri_Attention(c, c_z, ending = True)\n",
    "        \n",
    "        #define outer_product_mean\n",
    "        self.out_prod_mean = Outer_Prod_Mean(c, c_m, c_z, self.device)\n",
    "        \n",
    "    \n",
    "    def create_msa_rep(self, evos, seqs):\n",
    "        '''\n",
    "        Create the msa_representation given evolutionary data evos\n",
    "        and the seqs, both are n_res x 21.\n",
    "        '''\n",
    "        #obtain n_clust layers of PSSM(evos); stack them into a (n_clust x n_res x 256) matrix\n",
    "        clusters = [self.fc0[i](evos) for i in range(self.n_clust)]\n",
    "        msa_rep = torch.stack(clusters, dim=1)\n",
    "        \n",
    "        #project the seqs from n_res x 21 to n_res x 256 and tile it.\n",
    "        new_seqs = self.fc1(seqs)\n",
    "        new_seqs = new_seqs.unsqueeze(dim=1)\n",
    "        new_seqs = torch.tile(new_seqs, (1, self.n_clust, 1, 1))\n",
    "        \n",
    "        #add the seqs to the msa_rep\n",
    "        msa_rep += new_seqs\n",
    "        \n",
    "        return msa_rep\n",
    "    \n",
    "    def create_pair_rep(self, seqs):\n",
    "        '''\n",
    "        Create pair_wise representations given seqs.\n",
    "        '''\n",
    "        #create the pairwise rep matrix\n",
    "        a_i = self.fc2(seqs).unsqueeze(dim=2)\n",
    "        b_j = self.fc3(seqs).unsqueeze(dim=2)\n",
    "        a_i = torch.tile(a_i, (1, 1, a_i.shape[1], 1))\n",
    "        b_j = torch.tile(b_j, (1, 1, b_j.shape[1], 1))\n",
    "        pair_rep = a_i + torch.transpose(b_j, 1, 2)\n",
    "        \n",
    "        #add the relative position rel_pos\n",
    "        idx_j = torch.arange(0, seqs.shape[1]).unsqueeze(dim=1)\n",
    "        idx_j = torch.tile(idx_j, (1, idx_j.shape[1]))\n",
    "        idx_i = torch.transpose(idx_j, 0, 1)\n",
    "        # idx_i , idx_j = idx_i.to(device), idx_j.to(device)\n",
    "        dist_ij = idx_i - idx_j   \n",
    "        bins = torch.linspace(-32, 32, 64)\n",
    "        dist_ij = torch.bucketize(dist_ij, bins)\n",
    "        dist_ij[dist_ij>=64] = 63\n",
    "        dist_ij = dist_ij.unsqueeze(dim=0)\n",
    "        dist_ij = torch.tile(dist_ij, (pair_rep.shape[0], 1, 1))\n",
    "        dist_ij = F.one_hot(dist_ij).type(torch.float)\n",
    "        dist_ij = dist_ij.to(self.device)\n",
    "        rel_pos = self.fc4(dist_ij)\n",
    "        pair_rep += rel_pos\n",
    "        return pair_rep\n",
    "    \n",
    "    def create_bias(self, pair_rep):\n",
    "        '''\n",
    "        given the pairwise representation create the bias\n",
    "        '''\n",
    "        bias = self.fc5(pair_rep)\n",
    "        return bias\n",
    "        \n",
    "    def single_rep(self, msa_rep):\n",
    "        '''\n",
    "        Find the singular representation of M\n",
    "        Should only be done on the last block.\n",
    "        '''\n",
    "        single_rep = self.fc6(msa_rep[:,1,:,:])\n",
    "        single_rep = self.fc7(single_rep)\n",
    "        return single_rep  \n",
    "    \n",
    "    def forward(self, seqs, evos):\n",
    "        #create msa_rep, pair_rep, bias\n",
    "        msa_rep = self.create_msa_rep(evos, seqs)\n",
    "        pair_rep = self.create_pair_rep(seqs)\n",
    "        # bias = self.create_bias(pair_rep)\n",
    "        \n",
    "        # #feed msa_rep into row -> col -> transition\n",
    "        msa_rep = msa_rep + self.row_att(msa_rep, pair_rep) \n",
    "        msa_rep = msa_rep + self.col_att(msa_rep, pair_rep)\n",
    "        msa_rep = msa_rep + self.transition1(msa_rep) #output of evoformer for msa_rep\n",
    "        \n",
    "        #do the outer product mean\n",
    "        pair_rep = pair_rep + self.out_prod_mean(msa_rep, pair_rep)\n",
    "        \n",
    "        #do triangular attention\n",
    "        pair_rep = pair_rep + self.mul_att_out(pair_rep) \n",
    "        pair_rep = pair_rep + self.mul_att_in(pair_rep)\n",
    "        pair_rep = pair_rep + self.tri_att_start(pair_rep)\n",
    "        pair_rep = pair_rep + self.tri_att_end(pair_rep)\n",
    "        \n",
    "        #do the transition\n",
    "        pair_rep = pair_rep + self.transition2(pair_rep) #output of evoformer for pair_rep\n",
    "        \n",
    "        single_rep = self.single_rep(msa_rep)\n",
    "        return msa_rep, pair_rep, single_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f916c17-b347-4b83-988b-c593b1dc9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoformerBlock(nn.Module):\n",
    "    def __init__(self, c, c_m, c_z, num_blocks, n_clust, num_heads, device):\n",
    "        super(EvoformerBlock, self).__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks\n",
    "        self.evo_blocks = nn.ModuleList([Evoformer(c, c_m, c_z, n_clust, num_heads, device) for i in range(num_blocks)])\n",
    "        \n",
    "        #dmat\n",
    "        self.conv1 = nn.Conv2d(c_z, 256, 1)\n",
    "        \n",
    "        #angle \n",
    "        self.maxpool = nn.MaxPool2d((1,c_m))\n",
    "        self.conv2 = nn.Conv2d(c_z, 1296, 1)\n",
    "        \n",
    "    def forward(self, seqs, evos):\n",
    "        \n",
    "        output = self.evo_blocks[0](seqs, evos)\n",
    "        \n",
    "        #single rep is calculated but ignored until the last output\n",
    "        for i in range(1, self.num_blocks):\n",
    "            output = self.evo_blocks[0](output[0], output[1])\n",
    "        \n",
    "        #do convolutions to obtain the angle prediction\n",
    "        pred_dmat = self.conv1(torch.transpose(output[1], 1, 3))\n",
    "        \n",
    "        #obtain the angle predictions by maxpooling\n",
    "        pred_angles = self.maxpool(torch.transpose(output[1], 1, 3))  #shapes are b x cm x cm x c\n",
    "        pred_angles = self.conv2(pred_angles)\n",
    "        \n",
    "        return pred_dmat, pred_angles, output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33c7b74-6d7b-4367-8086-e5398fc856db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvoformerBlock(\n",
       "  (evo_blocks): ModuleList(\n",
       "    (0): Evoformer(\n",
       "      (fc0): ModuleList(\n",
       "        (0): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (3): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (4): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (5): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (6): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (7): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (8): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (9): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (10): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (11): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (12): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (13): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (14): Linear(in_features=21, out_features=256, bias=True)\n",
       "        (15): Linear(in_features=21, out_features=256, bias=True)\n",
       "      )\n",
       "      (fc1): Linear(in_features=20, out_features=256, bias=True)\n",
       "      (fc2): Linear(in_features=20, out_features=128, bias=True)\n",
       "      (fc3): Linear(in_features=20, out_features=128, bias=True)\n",
       "      (fc4): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (fc5): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (fc6): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (fc7): Linear(in_features=256, out_features=384, bias=True)\n",
       "      (transition1): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "      (transition2): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (row_att): rowColAtt(\n",
       "        (mhsa): ModuleList(\n",
       "          (0): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (1): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (2): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (3): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (4): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (5): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (6): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (7): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (gate): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (7): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (col_att): rowColAtt(\n",
       "        (mhsa): ModuleList(\n",
       "          (0): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (1): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (2): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (3): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (4): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (5): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (6): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "          (7): AttentionHead(\n",
       "            (q): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (k): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (v): Linear(in_features=256, out_features=16, bias=False)\n",
       "            (bias): Linear(in_features=128, out_features=1, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (gate): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (7): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mul_att_in): Mult_Attention(\n",
       "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (gate1): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (gate2): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (gate3): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (mul_att_out): Mult_Attention(\n",
       "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (gate1): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (gate2): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (gate3): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (tri_att_start): Tri_Attention(\n",
       "        (q): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (k): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (v): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (b): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "        )\n",
       "        (g): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "      (tri_att_end): Tri_Attention(\n",
       "        (q): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (k): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (v): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (b): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "        )\n",
       "        (g): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "            (1): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "      )\n",
       "      (out_prod_mean): Outer_Prod_Mean(\n",
       "        (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (fc2): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=(1, 256), stride=(1, 256), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(128, 1296, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "num_heads = 8\n",
    "n_clust = 16\n",
    "num_blocks = 1\n",
    "c = 16\n",
    "c_m = 256\n",
    "c_z = 128\n",
    "c_s = 256\n",
    "\n",
    "model = EvoformerBlock(c, c_m, c_z, num_blocks, n_clust, num_heads, device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a78894-7d51-4726-99b0-521cec455498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10110 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23933/2980996707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mdmat_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mang_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_crop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevo_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m#calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdmat_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmat_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23933/1447538899.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seqs, evos)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevo_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#single rep is calculated but ignored until the last output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23933/2688644109.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seqs, evos)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m#do triangular attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mpair_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_att_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mpair_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_att_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mpair_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri_att_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mpair_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtri_att_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23933/4238073577.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pair_rep)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgate_Z\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnew_B\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction = 'none')\n",
    "training_loss = 0.\n",
    "num_crops = 0\n",
    "for epoch in range(1,epochs+1):\n",
    "    for bidx, (batch) in enumerate(tqdm(data['train'])):\n",
    "        seqs, evos, angs, masks, dmats, dmat_masks = get_seq_features(batch)\n",
    "        seqs, evos, angs, masks, dmats, dmat_masks = seqs.to(device), evos.to(device), angs.to(device), masks.to(device), dmats.to(device), dmat_masks.to(device)\n",
    "        \n",
    "        #generate a random starting index\n",
    "        start_idx = random.randint(1,16)\n",
    "        \n",
    "        \n",
    "        original_shape = seqs.shape\n",
    "        seqs = F.pad(seqs, (0, 0, 0, seqs.shape[1] + c_s), 'constant', 0)\n",
    "        evos = F.pad(evos, (0, 0, 0, evos.shape[1] + c_s), 'constant', 0)\n",
    "        \n",
    "        #discretize the matrix\n",
    "        bins = torch.linspace(2,22, 64)\n",
    "        bins = bins.to(device)\n",
    "        discretized = torch.clamp(dmats, min = 2, max = 22)\n",
    "        discretized = torch.bucketize(discretized, bins, right = True)\n",
    "        discretized = F.pad(discretized, (0, discretized.shape[1] + c_s, 0, discretized.shape[1] + c_s, 0, 0), 'constant', 0)\n",
    "        \n",
    "        #discretize the angles\n",
    "        bins = torch.linspace(0, 36, 1296)\n",
    "        bins = bins.to(device)\n",
    "        d_angs = torch.clamp(angs, min = 0 , max = 36)\n",
    "        d_angs = d_angs.to(device)\n",
    "        d_angs = torch.bucketize(d_angs, bins, right = True)\n",
    "        d_angs = 36 * d_angs[:,:,0] + d_angs[:,:,1]\n",
    "        d_angs = F.pad(d_angs, (0, d_angs.shape[1] + c_s, 0, 0), 'constant', 0)\n",
    "        \n",
    "        for i in range(start_idx, original_shape[1], 128):\n",
    "            seq_crop = seqs[:,i:i+c_s,:]\n",
    "            evo_crop = evos[:,i:i+c_s,:]\n",
    "            ddmat = discretized[:,i:i+c_s, i:i+c_s]\n",
    "            new_angs = d_angs[:,i:i+c_s]\n",
    "            \n",
    "            #zero out previous gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            #forward pass\n",
    "            dmat_pred, ang_pred, single_rep = model(seq_crop.type(torch.float), evo_crop)\n",
    "            #calculate loss\n",
    "            dmat_loss = loss_func(dmat_pred, ddmat.long())\n",
    "            #angs_loss = loss_func(ang_pred.squeeze(dim=-1), new_angs.long())\n",
    "            loss = torch.mean(dmat_loss) #+ torch.mean(angs_loss)\n",
    "            training_loss += loss.item()\n",
    "            num_crops += 1\n",
    "            #backpropagate and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if bidx % 100 == 0:\n",
    "            checkpoint = {\n",
    "                'batch': bidx,\n",
    "                'epoch': epoch,\n",
    "                'loss': training_loss,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, \"alphafold2.pth\")\n",
    "    training_loss /= num_crops\n",
    "    with open('output.txt', 'a') as fp:\n",
    "        fp.write(f\"Epoch: {epoch} Loss: {training_loss}\\n\")\n",
    "        print(f\"Epoch: {epoch} Loss: {training_loss}\")\n",
    "    fp.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae8ad6-e7f1-466c-9d69-bb9415c6ca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
